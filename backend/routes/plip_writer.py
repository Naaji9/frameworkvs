# backend/api/routers/plip_writer.py
"""
PLIP Standalone Script Generator
Generates a self-contained Python script for PLIP analysis
that can run anywhere without backend dependencies
"""

import os
from fastapi import APIRouter, Form, HTTPException
from fastapi.responses import Response
from typing import Optional


router = APIRouter(prefix="/plip", tags=["PLIP Script Generator"])


@router.post("/generate-script")
async def generate_script_endpoint(
    receptor_path: str = Form(...),
    ligand_path: str = Form(...),
    output_folder: str = Form(...),
    max_poses: int = Form(5),
    max_workers: int = Form(4),
    remove_waters: bool = Form(False),
    remove_ions: bool = Form(False),
    add_hydrogens: bool = Form(True),
    keep_hetero: bool = Form(True)
):
    """
    API endpoint to generate standalone PLIP script
    """
    try:
        script_content = generate_standalone_script(
            receptor_path=receptor_path,
            ligand_path=ligand_path,
            output_folder=output_folder,
            max_poses=max_poses,
            max_workers=max_workers,
            remove_waters=remove_waters,
            remove_ions=remove_ions,
            add_hydrogens=add_hydrogens,
            keep_hetero=keep_hetero
        )
        
        return Response(
            content=script_content,
            media_type="text/x-python",
            headers={
                "Content-Disposition": "attachment; filename=plip_analysis_standalone.py"
            }
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def generate_standalone_script(
    receptor_path: str,
    ligand_path: str,
    output_folder: str,
    max_poses: int = 5,
    max_workers: int = 4,
    remove_waters: bool = False,
    remove_ions: bool = False,
    add_hydrogens: bool = True,
    keep_hetero: bool = True
) -> str:
    """
    Generate a complete standalone PLIP analysis script
    """
    
    script = f'''#!/usr/bin/env python3
"""
PLIP Analysis - Standalone Script
Generated by FrameworkVS 3.0

This script is completely self-contained and can run on any machine
with Python 3.6+ and the required dependencies installed.

Dependencies Required:
  - PLIP (https://github.com/pharmai/plip)
  - Open Babel (obabel command)

Installation:
  pip install plip
  # For obabel, see: http://openbabel.org/wiki/Category:Installation
"""

import os
import sys
import tempfile
import shutil
import subprocess
import json
import csv
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import xml.etree.ElementTree as ET

# ============================================================================
# CONFIGURATION - User Parameters
# ============================================================================

RECEPTOR_PATH = r"{receptor_path}"
LIGAND_PATH = r"{ligand_path}"
OUTPUT_FOLDER = r"{output_folder}"
MAX_POSES = {max_poses}
MAX_WORKERS = {max_workers}
REMOVE_WATERS = {str(remove_waters)}
REMOVE_IONS = {str(remove_ions)}
ADD_HYDROGENS = {str(add_hydrogens)}
KEEP_HETERO = {str(keep_hetero)}

# ============================================================================
# DEPENDENCY VERIFICATION
# ============================================================================

def check_dependencies():
    """Verify required dependencies are installed"""
    print("\\n" + "="*80)
    print("üîç CHECKING DEPENDENCIES")
    print("="*80)
    
    missing = []
    
    # Check PLIP
    try:
        result = subprocess.run(["plip", "-h"], 
                              capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            # Extract version from help text
            version_line = [line for line in result.stdout.split('\\n') if 'Version' in line]
            version = version_line[0].strip() if version_line else "installed"
            print(f"  ‚úÖ PLIP found: {{version}}")
        else:
            missing.append("PLIP")
    except (FileNotFoundError, subprocess.TimeoutExpired):
        missing.append("PLIP")
        print("  ‚ùå PLIP not found")
    
    # Check Open Babel
    try:
        result = subprocess.run(["obabel", "-V"], 
                              capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            version = result.stdout.strip().split('\\n')[0]
            print(f"  ‚úÖ Open Babel found: {{version}}")
        else:
            missing.append("Open Babel")
    except (FileNotFoundError, subprocess.TimeoutExpired):
        missing.append("Open Babel")
        print("  ‚ùå Open Babel not found")
    
    if missing:
        print("\\n" + "="*80)
        print("‚ùå MISSING DEPENDENCIES")
        print("="*80)
        print("\\nThe following dependencies are required but not found:")
        for dep in missing:
            print(f"  ‚Ä¢ {{dep}}")
        
        print("\\nüì¶ Installation Instructions:")
        print("\\n1. Install PLIP:")
        print("   pip install plip")
        print("\\n2. Install Open Babel:")
        print("   - Ubuntu/Debian: sudo apt-get install openbabel")
        print("   - macOS: brew install open-babel")
        print("   - Windows: Download from http://openbabel.org/wiki/Category:Installation")
        print("   - Or via conda: conda install -c conda-forge openbabel")
        print("\\n" + "="*80)
        sys.exit(1)
    
    print("="*80)
    print("‚úÖ All dependencies satisfied!\\n")



# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def safe_run(cmd, cwd=None):
    """Run command and handle errors"""
    p = subprocess.run(cmd, cwd=cwd, stdout=subprocess.PIPE, 
                      stderr=subprocess.PIPE, text=True)
    if p.returncode != 0:
        raise Exception(f"Failed: {{' '.join(cmd)}}\\n{{p.stderr}}")
    return p.stdout


def remove_smiles_from_pdb(pdb_path):
    """Remove SMILES strings from PDB file"""
    temp_path = pdb_path + ".tmp"
    lines_removed = 0
    
    try:
        with open(pdb_path, "r") as infile, open(temp_path, "w") as outfile:
            for line in infile:
                stripped = line.strip()
                is_smiles = False
                
                if stripped.startswith("SMILES") or stripped.lower().startswith("smiles:"):
                    is_smiles = True
                elif "[C@@H]" in line or "[C@H]" in line or "[@" in line:
                    is_smiles = True
                elif stripped and not stripped.startswith(("ATOM", "HETATM", "TER", "END", 
                                                          "MODEL", "ENDMDL", "CONECT", 
                                                          "REMARK", "HEADER", "TITLE", "CRYST")):
                    if len(stripped) > 50 and stripped.count("(") + stripped.count("[") > 5 and stripped.count(" ") < 3:
                        is_smiles = True
                
                if is_smiles:
                    lines_removed += 1
                    continue
                    
                outfile.write(line)
        
        shutil.move(temp_path, pdb_path)
        
        if lines_removed > 0:
            print(f"    ‚úì Removed {{lines_removed}} SMILES lines from PDB")
        
        return True
    except Exception as e:
        print(f"    ‚ö† Warning: Could not clean SMILES from PDB: {{e}}")
        if os.path.exists(temp_path):
            os.remove(temp_path)
        return False


def split_pdbqt_models(src, out_dir):
    """Split multi-model PDBQT into individual pose files"""
    poses = []
    model = []
    idx = 0
    with open(src, "r") as f:
        for line in f:
            if line.startswith("MODEL"):
                model = []
            model.append(line)
            if line.startswith("ENDMDL"):
                idx += 1
                out = os.path.join(out_dir, f"pose_{{idx}}.pdbqt")
                with open(out, "w") as o:
                    o.writelines(model)
                poses.append(out)
                model = []
    if model:
        idx += 1
        out = os.path.join(out_dir, f"pose_{{idx}}.pdbqt")
        with open(out, "w") as o:
            o.writelines(model)
        poses.append(out)
    return poses


def merge_pdbqt(rec, lig, outp):
    """Merge receptor and ligand into single PDB file"""
    with open(outp, "w") as out:
        out.write(open(rec).read())
        out.write(open(lig).read())


def parse_plip_xml(xml_path, output_dir):
    """Parse PLIP XML output and create CSV/JSON files"""
    if not os.path.exists(xml_path):
        return False
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()
        all_interactions = []
        interactions_by_type = {{}}
        
        for bindingsite in root.findall('.//bindingsite'):
            interactions_node = bindingsite.find('interactions')
            if not interactions_node:
                continue
            identifiers = bindingsite.find('identifiers')
            site_info = {{
                'ligand_id': identifiers.find('hetid').text if identifiers.find('hetid') is not None else '',
                'chain': identifiers.find('chain').text if identifiers.find('chain') is not None else '',
                'position': identifiers.find('position').text if identifiers.find('position') is not None else ''
            }}
            
            interaction_map = {{
                'hydrophobic_interactions': 'hydrophobic_interaction',
                'hydrogen_bonds': 'hydrogen_bond',
                'water_bridges': 'water_bridge',
                'salt_bridges': 'salt_bridge',
                'pi_stacks': 'pi_stack',
                'pi_cation_interactions': 'pi_cation_interaction',
                'halogen_bonds': 'halogen_bond',
                'metal_complexes': 'metal_complex'
            }}
            
            for plural, singular in interaction_map.items():
                coll = interactions_node.find(plural)
                if not coll:
                    continue
                interaction_type_name = plural.replace('_', ' ').title()
                if interaction_type_name not in interactions_by_type:
                    interactions_by_type[interaction_type_name] = []
                for interaction in coll.findall(singular):
                    data = {{**site_info, 'interaction_type': interaction_type_name}}
                    for child in interaction:
                        if child.text and child.text.strip():
                            data[child.tag] = child.text.strip()
                        elif len(child) > 0:
                            for subchild in child:
                                if subchild.text and subchild.text.strip():
                                    data[f"{{child.tag}}_{{subchild.tag}}"] = subchild.text.strip()
                    all_interactions.append(data)
                    interactions_by_type[interaction_type_name].append(data)
        
        if all_interactions:
            csv_path = os.path.join(output_dir, 'interactions_all.csv')
            preferred_order = [
                'interaction_type', 'dist', 'dist_d-a', 'dist_h-a', 'don_angle', 'donoridx',
                'donortype', 'restype', 'resnr', 'acceptoridx', 'acceptortype',
            ]
            all_keys = sorted(set(k for d in all_interactions for k in d.keys()))
            ordered_keys = [k for k in preferred_order if k in all_keys] + \\
                           [k for k in all_keys if k not in preferred_order]
            keys = ordered_keys

            with open(csv_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=keys)
                writer.writeheader()
                writer.writerows(all_interactions)
            
            for itype, interactions in interactions_by_type.items():
                if not interactions:
                    continue
                filename = f"{{itype.replace(' ', '_')}}.csv"
                with open(os.path.join(output_dir, filename), 'w', newline='', encoding='utf-8') as f:
                    type_keys = sorted(set(k for d in interactions for k in d.keys()))
                    writer = csv.DictWriter(f, fieldnames=type_keys)
                    writer.writeheader()
                    writer.writerows(interactions)
            
            with open(os.path.join(output_dir, 'interactions_all.json'), 'w') as f:
                json.dump(all_interactions, f, indent=2)
            
            for itype, interactions in interactions_by_type.items():
                if not interactions:
                    continue
                filename = f"{{itype.replace(' ', '_')}}.json"
                with open(os.path.join(output_dir, filename), 'w') as f:
                    json.dump(interactions, f, indent=2)
            
            summary_path = os.path.join(output_dir, 'interaction_summary.csv')
            counts = {{}}
            for i in all_interactions:
                itype = i.get('interaction_type', 'Unknown')
                counts[itype] = counts.get(itype, 0) + 1
            with open(summary_path, 'w', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(['Interaction Type', 'Count'])
                for itype, count in sorted(counts.items()):
                    writer.writerow([itype, count])
            return True
        return False
    except Exception as e:
        print(f"    ‚ö† Error parsing XML: {{e}}")
        return False


# ============================================================================
# MATCHING LOGIC
# ============================================================================

def get_receptor_identifiers(receptor_filename):
    """Extract possible identifiers from receptor name"""
    base = receptor_filename.replace('.pdbqt', '').replace('.pdb', '')
    identifiers = [base]
    
    if '_' in base:
        parts = base.split('_')
        for i in range(len(parts)):
            prefix = '_'.join(parts[:i+1])
            if prefix and prefix not in identifiers:
                identifiers.append(prefix)
        
        if parts[0] not in identifiers:
            identifiers.append(parts[0])
    
    identifiers.sort(key=len, reverse=True)
    return identifiers


def separate_receptors_and_ligands(all_files):
    """
    Intelligently separate files into receptors and ligands based on patterns
    
    Logic:
    - Ligands (docking results) typically have pattern: receptor_compound_score_...
    - Receptors are simpler: protein_name.pdbqt
    - Ligands usually have more underscores and numeric scores
    """
    receptors = []
    ligands = []
    
    print(f"\\n{{'='*80}}")
    print(f"üîç INTELLIGENT FILE SEPARATION")
    print(f"{{'='*80}}")
    
    for file_path in all_files:
        filename = os.path.basename(file_path)
        base_name = filename.replace('.pdbqt', '').replace('.pdb', '')
        
        # Criteria for identifying ligands (docking results):
        # 1. Has negative numbers (scores like -7.36)
        # 2. Has 3+ underscores (receptor_compound_score_...)
        # 3. Has multiple decimal numbers
        
        has_negative_number = '-' in base_name and any(c.isdigit() for c in base_name.split('-')[-1][:3])
        underscore_count = base_name.count('_')
        decimal_count = base_name.count('.')
        
        # Ligand detection logic
        is_ligand = (
            has_negative_number or  # Has docking score
            underscore_count >= 3 or  # Complex naming with multiple parts
            (underscore_count >= 2 and decimal_count >= 2)  # Has coordinates/scores
        )
        
        if is_ligand:
            ligands.append(file_path)
            print(f"  üìä LIGAND: {{filename}}")
        else:
            receptors.append(file_path)
            print(f"  üß¨ RECEPTOR: {{filename}}")
    
    print(f"{{'='*80}}")
    print(f"  Total Receptors: {{len(receptors)}}")
    print(f"  Total Ligands: {{len(ligands)}}")
    print(f"{{'='*80}}\\n")
    
    return receptors, ligands


def match_receptors_to_ligands(receptor_files, ligand_files):
    """Match receptors to ligands using flexible prefix matching"""
    combinations = []
    skipped_receptors = []
    
    print(f"{{'='*80}}")
    print(f"üîó MATCHING PROCESS STARTED (Flexible Prefix Matching)")
    print(f"{{'='*80}}")
    print(f"  Total receptors to process: {{len(receptor_files)}}")
    print(f"  Total ligands available: {{len(ligand_files)}}")
    print(f"{{'='*80}}\\n")
    
    for idx, receptor_path in enumerate(receptor_files, 1):
        receptor_name = os.path.basename(receptor_path)
        receptor_base = receptor_name.replace('.pdbqt', '').replace('.pdb', '')
        
        print(f"‚îå{{'‚îÄ'*78}}‚îê")
        print(f"‚îÇ[{{idx}}/{{len(receptor_files)}}] Processing Receptor: {{receptor_name:<60}}‚îÇ")
        print(f"‚îî{{'‚îÄ'*78}}‚îò")
        print(f"  Full path: {{receptor_path}}")
        print(f"  Base name: {{receptor_base}}")
        print(f"  Checking {{len(ligand_files)}} ligand(s)...\\n")
        
        matches_found = 0
        
        for ligand_path in ligand_files:
            ligand_name = os.path.basename(ligand_path)
            ligand_base = ligand_name.replace('.pdbqt', '').replace('.pdb', '')
            
            matched = False
            match_type = ""
            
            # METHOD 1: Exact prefix match with underscore
            if ligand_base.startswith(f"{{receptor_base}}_"):
                matched = True
                match_type = f"exact: '{{receptor_base}}_'"
            
            # METHOD 2: Flexible matching - first 8+ characters match
            elif len(receptor_base) >= 8:
                for min_chars in [12, 11, 10, 9, 8]:  # Try longer matches first
                    if len(receptor_base) >= min_chars:
                        prefix = receptor_base[:min_chars]
                        if ligand_base.startswith(prefix):
                            matched = True
                            match_type = f"flexible: first {{min_chars}} chars '{{prefix}}'"
                            break
            
            if matched:
                combinations.append({{
                    "receptor_path": receptor_path,
                    "receptor_name": receptor_name,
                    "ligand_path": ligand_path,
                    "ligand_name": ligand_name,
                    "matched_identifier": receptor_base
                }})
                matches_found += 1
                print(f"    ‚úÖ Match #{{matches_found}}: {{ligand_name}}")
                print(f"       Matched via: {{match_type}}")
        
        print(f"\\n  {{'‚îÄ'*76}}")
        if matches_found == 0:
            print(f"  ‚ö†Ô∏è  NO MATCHES FOUND for {{receptor_name}}")
            print(f"  This receptor will be SKIPPED")
            skipped_receptors.append(receptor_name)
        else:
            print(f"  ‚úÖ Total matches for {{receptor_name}}: {{matches_found}}")
        print(f"  {{'‚îÄ'*76}}\\n")
    
    print(f"{{'='*80}}")
    print(f"‚úÖ MATCHING PROCESS COMPLETE")
    print(f"{{'='*80}}")
    print(f"  Total combinations created: {{len(combinations)}}")
    print(f"  Receptors successfully matched: {{len(receptor_files) - len(skipped_receptors)}}")
    print(f"  Receptors skipped (no matches): {{len(skipped_receptors)}}")
    print(f"{{'='*80}}\\n")
    
    if skipped_receptors:
        print(f"‚ö†Ô∏è  The following receptor(s) had NO matching ligands:")
        for i, rec in enumerate(skipped_receptors, 1):
            print(f"    {{i}}. {{rec}}")
        print()
    
    return combinations


def get_all_pdbqt_files(path):
    """Get all .pdb and .pdbqt files from path recursively (file or directory)"""
    # Handle single file
    if os.path.isfile(path):
        if path.endswith(('.pdb', '.pdbqt')):
            print(f"  üìÑ Single file detected: {{os.path.basename(path)}}")
            return [path]
        else:
            print(f"  ‚ö†Ô∏è  WARNING: {{path}} is not a PDB/PDBQT file")
            return []
    
    # Handle directory - RECURSIVE SEARCH
    if os.path.isdir(path):
        files = []
        try:
            print(f"  üìÅ Searching recursively in: {{path}}")
            for root, dirs, filenames in os.walk(path):
                for fname in filenames:
                    if fname.endswith(('.pdb', '.pdbqt')):
                        full_path = os.path.join(root, fname)
                        files.append(full_path)
                        # Show subdirectory being searched
                        rel_path = os.path.relpath(root, path)
                        if rel_path != '.':
                            print(f"     Found in: {{rel_path}}/{{fname}}")
            
            if files:
                print(f"  ‚úÖ Total found: {{len(files)}} PDB/PDBQT file(s) across all subdirectories")
            else:
                print(f"  ‚ö†Ô∏è  No PDB/PDBQT files found in {{path}} or its subdirectories")
            return sorted(files)
        except Exception as e:
            print(f"  ‚ùå ERROR reading directory {{path}}: {{e}}")
            return []
    
    # Path doesn't exist - try parent directory
    print(f"  ‚ö†Ô∏è  WARNING: Path does not exist: {{path}}")
    parent = os.path.dirname(path)
    
    if parent and os.path.isdir(parent):
        print(f"  üîç Searching recursively in parent directory: {{parent}}")
        files = []
        for root, dirs, filenames in os.walk(parent):
            for fname in filenames:
                if fname.endswith(('.pdb', '.pdbqt')):
                    full_path = os.path.join(root, fname)
                    files.append(full_path)
        if files:
            print(f"  ‚úÖ Found {{len(files)}} PDB/PDBQT file(s) in parent directory tree!")
            return sorted(files)
        else:
            print(f"  ‚ùå No PDB/PDBQT files found in parent directory tree")
    
    print(f"  ‚ùå ERROR: Could not find any valid files")
    return []

# ============================================================================
# SINGLE COMBINATION PROCESSING
# ============================================================================

def execute_single_combination(combo, max_poses):
    """Execute PLIP analysis for a single receptor-ligand combination"""
    combo_output_dir = combo["output_dir"]
    
    # Copy original files
    original_files_dir = os.path.join(combo_output_dir, "original_files")
    os.makedirs(original_files_dir, exist_ok=True)
    
    receptor_copy = os.path.join(original_files_dir, combo['receptor_name'])
    ligand_copy = os.path.join(original_files_dir, combo['ligand_name'])
    
    shutil.copy2(combo["receptor_path"], receptor_copy)
    shutil.copy2(combo["ligand_path"], ligand_copy)
    
    # Split ligand file into individual poses
    poses_root = os.path.join(combo_output_dir, "poses")
    os.makedirs(poses_root, exist_ok=True)
    all_pose_files = split_pdbqt_models(combo["ligand_path"], poses_root)
    
    total_poses = len(all_pose_files)
    selected_poses = all_pose_files[:max_poses]
    
    print(f"    Total poses in ligand: {{total_poses}}")
    print(f"    Analyzing first {{len(selected_poses)}} pose(s)")
    
    # Remove excess pose files
    for pose_file in all_pose_files[max_poses:]:
        try:
            os.remove(pose_file)
        except:
            pass
    
    # Process each pose
    pose_results = []
    for i, pose_file in enumerate(selected_poses, start=1):
        print(f"\\n    üéØ Processing pose {{i}}/{{len(selected_poses)}}...")
        pose_dir = os.path.join(combo_output_dir, f"pose_{{i}}")
        os.makedirs(pose_dir, exist_ok=True)
        
        merge_path = os.path.join(pose_dir, f"merge_{{i}}.pdbqt")
        complex_path = os.path.join(pose_dir, "complex.pdb")
        
        # Merge receptor and ligand
        merge_pdbqt(combo["receptor_path"], pose_file, merge_path)
        
        # Convert to PDB format
        safe_run(["obabel", merge_path, "-O", complex_path], cwd=pose_dir)
        
        # Remove SMILES from PDB
        print(f"      Cleaning SMILES from complex.pdb...")
        remove_smiles_from_pdb(complex_path)
        
        # Run PLIP analysis
        plip_cmd = ["plip", "-f", complex_path, "-x", "-t", "-y", 
                   "--nohydro", "--nofixfile", "--nofix"]
        try:
            safe_run(plip_cmd, cwd=pose_dir)
            xml_path = os.path.join(pose_dir, "report.xml")
            if os.path.exists(xml_path):
                parse_plip_xml(xml_path, pose_dir)
                print(f"      ‚úì Pose {{i}} completed")
            else:
                print(f"      ‚ö† Pose {{i}}: No XML output")
        except Exception as e:
            print(f"      ‚úó Pose {{i}} failed: {{e}}")
        
        # Collect output files
        files = os.listdir(pose_dir)
        pose_results.append({{
            "pose": i,
            "folder": pose_dir,
            "csv_files": [f for f in files if f.endswith('.csv')],
            "json_files": [f for f in files if f.endswith('.json')],
            "png_files": [f for f in files if f.endswith('.png')],
            "xml_files": [f for f in files if f.endswith('.xml')],
            "txt_files": [f for f in files if f.endswith('.txt')],
            "pse_files": [f for f in files if f.endswith('.pse')],
            "pml_files": [f for f in files if f.endswith('.pml')],
            "pdb_files": [f for f in files if f.endswith('.pdb')],
            "pdbqt_files": [f for f in files if f.endswith('.pdbqt')],
        }})
    
    return {{
        "combo_name": combo["combo_name"],
        "receptor": combo["receptor_name"],
        "ligand": combo["ligand_name"],
        "output_dir": combo_output_dir,
        "total_poses_in_file": total_poses,
        "poses_analyzed": len(selected_poses),
        "poses": pose_results
    }}


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    print("\\n" + "="*80)
    print("üß¨ PLIP ANALYSIS - STANDALONE SCRIPT")
    print("="*80)
    print("Generated by: Combi VS FrameworkVS 3.0")
    print("="*80)
    
    # Check dependencies first
    check_dependencies()
    
    # Validate paths
    print("\\n" + "="*80)
    print("üìÇ VALIDATING PATHS")
    print("="*80)
    
    if not os.path.exists(RECEPTOR_PATH):
        print(f"‚ùå Receptor path does not exist: {{RECEPTOR_PATH}}")
        sys.exit(1)
    print(f"  ‚úÖ Receptor: {{RECEPTOR_PATH}}")
    
    if not os.path.exists(LIGAND_PATH):
        print(f"‚ùå Ligand path does not exist: {{LIGAND_PATH}}")
        sys.exit(1)
    print(f"  ‚úÖ Ligand: {{LIGAND_PATH}}")
    
    # Create output folder
    os.makedirs(OUTPUT_FOLDER, exist_ok=True)
    print(f"  ‚úÖ Output: {{OUTPUT_FOLDER}}")
    print("="*80)
    
    # Display configuration
    print("\\n" + "="*80)
    print("‚öôÔ∏è  CONFIGURATION")
    print("="*80)
    print(f"  Max poses per combination: {{MAX_POSES}}")
    print(f"  Parallel workers: {{MAX_WORKERS}}")
    print(f"  Remove waters: {{REMOVE_WATERS}}")
    print(f"  Remove ions: {{REMOVE_IONS}}")
    print(f"  Add hydrogens: {{ADD_HYDROGENS}}")
    print(f"  Keep heteroatoms: {{KEEP_HETERO}}")
    print("="*80)
    
    # Get receptor and ligand files
# Get all PDB/PDBQT files recursively
    print("\\n" + "="*80)
    print("üìÅ SCANNING FILES RECURSIVELY")
    print("="*80)
    
    # Combine both paths for searching
    search_paths = set()
    if os.path.exists(RECEPTOR_PATH):
        search_paths.add(os.path.abspath(RECEPTOR_PATH))
    if os.path.exists(LIGAND_PATH):
        search_paths.add(os.path.abspath(LIGAND_PATH))
    
    all_files = []
    for path in search_paths:
        files = get_all_pdbqt_files(path)
        all_files.extend(files)
    
    # Remove duplicates while preserving order
    seen = set()
    unique_files = []
    for f in all_files:
        if f not in seen:
            seen.add(f)
            unique_files.append(f)
    all_files = unique_files
    
    if not all_files:
        print("‚ùå No PDB/PDBQT files found")
        sys.exit(1)
    
    # Intelligently separate receptors from ligands
    receptor_files, ligand_files = separate_receptors_and_ligands(all_files)
    
    print(f"  Receptors found: {{len(receptor_files)}}")
    for r in receptor_files:
        print(f"    - {{os.path.basename(r)}}")
    
    print(f"  Ligands found: {{len(ligand_files)}}")
    for l in ligand_files:
        print(f"    - {{os.path.basename(l)}}")
    
    print("="*80)
    
    if not receptor_files:
        print("‚ùå No receptor files found")
        sys.exit(1)
    if not ligand_files:
        print("‚ùå No ligand files found")
        sys.exit(1)
    
    # Match receptors with ligands
    matched_combinations = match_receptors_to_ligands(receptor_files, ligand_files)
    
    if not matched_combinations:
        print("‚ùå No matching receptor-ligand pairs found!")
        print("   Ensure ligands are named like: {{receptor_prefix}}_{{compound}}_{{score}}.pdbqt")
        sys.exit(1)
    
    # Create job directory
    job_id = os.urandom(4).hex()
    job_dir = os.path.join(OUTPUT_FOLDER, f"plip_job_{{job_id}}")
    os.makedirs(job_dir, exist_ok=True)
    
    print(f"\\nüìÅ Job directory created: {{job_dir}}")
    
    # Save matching summary
    summary_path = os.path.join(job_dir, "combinations_summary.json")
    with open(summary_path, 'w') as f:
        json.dump({{
            "total_combinations": len(matched_combinations),
            "combinations": [
                {{
                    "receptor": c["receptor_name"],
                    "ligand": c["ligand_name"],
                    "matched_identifier": c["matched_identifier"]
                }}
                for c in matched_combinations
            ]
        }}, f, indent=2)
    print(f"‚úì Saved combinations summary: {{summary_path}}\\n")
    
    # Prepare output directories for each combination
    for combo in matched_combinations:
        combo_name = f"{{combo['receptor_name'].replace('.pdbqt', '').replace('.pdb', '')}}_{{combo['ligand_name'].replace('.pdbqt', '').replace('.pdb', '')}}"
        combo_dir = os.path.join(job_dir, combo_name)
        os.makedirs(combo_dir, exist_ok=True)
        combo["output_dir"] = combo_dir
        combo["combo_name"] = combo_name
    
    # Execute combinations in parallel
    print(f" Processing {{len(matched_combinations)}} combination(s) in parallel...")
    print(f"   Using {{MAX_WORKERS}} worker(s)\\n")
    
    start_time = time.time()
    results = []
    failed = []
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_combo = {{
            executor.submit(execute_single_combination, combo, MAX_POSES): combo
            for combo in matched_combinations
        }}
        
        for i, future in enumerate(as_completed(future_to_combo), 1):
            combo = future_to_combo[future]
            try:
                result = future.result()
                results.append(result)
                print(f"[{{i}}/{{len(matched_combinations)}}] ‚úÖ {{combo['combo_name']}}")
            except Exception as e:
                error_info = {{
                    "combo_name": combo['combo_name'],
                    "receptor": combo['receptor_name'],
                    "ligand": combo['ligand_name'],
                    "error": str(e)
                }}
                failed.append(error_info)
                print(f"[{{i}}/{{len(matched_combinations)}}] ‚ùå {{combo['combo_name']}} - {{e}}")
    
    elapsed_time = time.time() - start_time
    
    # Final summary
    print(f"\\n{{'='*80}}")
    print(f"üéâ JOB COMPLETE!")
    print(f"{{'='*80}}")
    print(f"  Job ID: {{job_id}}")
    print(f"  Job Directory: {{job_dir}}")
    print(f"  Total combinations: {{len(matched_combinations)}}")
    print(f"  ‚úÖ Successful: {{len(results)}}")
    print(f"  ‚ùå Failed: {{len(failed)}}")
    print(f"  ‚è±Ô∏è  Time: {{elapsed_time:.2f}}s")
    print(f"{'='*80}")
    
    if failed:
        print(f"\\n‚ö†Ô∏è  Failed combinations:")
        for f in failed:
            print(f"  - {{f['combo_name']}}: {{f['error']}}")
        print()
    
    # Save final results
    final_results_path = os.path.join(job_dir, "final_results.json")
    final_results = {{
        "job_id": job_id,
        "job_directory": job_dir,
        "start_time": start_time,
        "elapsed_time": elapsed_time,
        "total_combinations": len(matched_combinations),
        "successful": len(results),
        "failed": len(failed),
        "configuration": {{
            "receptor_path": RECEPTOR_PATH,
            "ligand_path": LIGAND_PATH,
            "output_folder": OUTPUT_FOLDER,
            "max_poses": MAX_POSES,
            "max_workers": MAX_WORKERS,
            "remove_waters": REMOVE_WATERS,
            "remove_ions": REMOVE_IONS,
            "add_hydrogens": ADD_HYDROGENS,
            "keep_hetero": KEEP_HETERO
        }},
        "results": results,
        "failed_combinations": failed
    }}
    
    with open(final_results_path, 'w') as f:
        json.dump(final_results, f, indent=2)
    
    print(f"\\n Results saved to: {{final_results_path}}")
    
    # Print summary of output files
    print(f"\\nüìä Output Summary:")
    total_csv = sum(len(pose.get('csv_files', [])) for r in results for pose in r.get('poses', []))
    total_json = sum(len(pose.get('json_files', [])) for r in results for pose in r.get('poses', []))
    total_png = sum(len(pose.get('png_files', [])) for r in results for pose in r.get('poses', []))
    
    print(f"  CSV files: {{total_csv}}")
    print(f"  JSON files: {{total_json}}")
    print(f"  PNG files: {{total_png}}")
    
    print(f"\\n‚úÖ Analysis complete! Check the job directory for all results.")
    print(f"   {{job_dir}}\\n")
    print("Generated by: Combi VS FrameworkVS 3.0")


if __name__ == "__main__":
    main()
'''
    
    return script
