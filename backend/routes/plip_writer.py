# backend/api/routers/plip_writer.py
"""
PLIP Standalone Script Generator
Generates a self-contained Python script for PLIP analysis
that can run anywhere without backend dependencies
"""

import os
from fastapi import APIRouter, Form, HTTPException
from fastapi.responses import Response
from typing import Optional


router = APIRouter(prefix="/plip", tags=["PLIP Script Generator"])


@router.post("/generate-script")
async def generate_script_endpoint(
    receptor_path: str = Form(...),
    ligand_path: str = Form(...),
    output_folder: str = Form(...),
    max_poses: int = Form(5),
    max_workers: int = Form(4),
    remove_waters: bool = Form(False),
    remove_ions: bool = Form(False),
    add_hydrogens: bool = Form(True),
    keep_hetero: bool = Form(True)
):
    """
    API endpoint to generate standalone PLIP script
    """
    try:
        script_content = generate_standalone_script(
            receptor_path=receptor_path,
            ligand_path=ligand_path,
            output_folder=output_folder,
            max_poses=max_poses,
            max_workers=max_workers,
            remove_waters=remove_waters,
            remove_ions=remove_ions,
            add_hydrogens=add_hydrogens,
            keep_hetero=keep_hetero
        )
        
        return Response(
            content=script_content,
            media_type="text/x-python",
            headers={
                "Content-Disposition": "attachment; filename=plip_analysis_standalone.py"
            }
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def generate_standalone_script(
    receptor_path: str,
    ligand_path: str,
    output_folder: str,
    max_poses: int = 5,
    max_workers: int = 4,
    remove_waters: bool = False,
    remove_ions: bool = False,
    add_hydrogens: bool = True,
    keep_hetero: bool = True
) -> str:
    """
    Generate a complete standalone PLIP analysis script
    """
    
    script = f'''#!/usr/bin/env python3
"""
PLIP Analysis - Standalone Script
Generated by FrameworkVS 3.0

This script is completely self-contained and can run on any machine
with Python 3.6+ and the required dependencies installed.

Dependencies Required:
  - PLIP (https://github.com/pharmai/plip)
  - Open Babel (obabel command)

Installation:
  pip install plip
  # For obabel, see: http://openbabel.org/wiki/Category:Installation
"""

import os
import sys
import tempfile
import shutil
import subprocess
import json
import csv
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import xml.etree.ElementTree as ET

# ============================================================================
# CONFIGURATION - User Parameters
# ============================================================================

RECEPTOR_PATH = r"{receptor_path}"
LIGAND_PATH = r"{ligand_path}"
OUTPUT_FOLDER = r"{output_folder}"
MAX_POSES = {max_poses}
MAX_WORKERS = {max_workers}
REMOVE_WATERS = {str(remove_waters)}
REMOVE_IONS = {str(remove_ions)}
ADD_HYDROGENS = {str(add_hydrogens)}
KEEP_HETERO = {str(keep_hetero)}

# ============================================================================
# DEPENDENCY VERIFICATION
# ============================================================================

def check_dependencies():
    """Verify required dependencies are installed"""
    print("\\n" + "="*80)
    print(" CHECKING DEPENDENCIES")
    print("="*80)
    
    missing = []
    
    # Check PLIP
    try:
        result = subprocess.run(["plip", "-h"], 
                              capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            # Extract version from help text
            version_line = [line for line in result.stdout.split('\\n') if 'Version' in line]
            version = version_line[0].strip() if version_line else "installed"
            print(f"  ‚úîÔ∏è  PLIP found: {{version}}")
        else:
            missing.append("PLIP")
    except (FileNotFoundError, subprocess.TimeoutExpired):
        missing.append("PLIP")
        print("  ‚úñÔ∏è PLIP not found")
    
    # Check Open Babel
    try:
        result = subprocess.run(["obabel", "-V"], 
                              capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            version = result.stdout.strip().split('\\n')[0]
            print(f"  ‚úì Open Babel found: {{version}}")
        else:
            missing.append("Open Babel")
    except (FileNotFoundError, subprocess.TimeoutExpired):
        missing.append("Open Babel")
        print("  ‚úñÔ∏è Open Babel not found")
    
    if missing:
        print("\\n" + "="*80)
        print("‚úñÔ∏è MISSING DEPENDENCIES")
        print("="*80)
        print("\\nThe following dependencies are required but not found:")
        for dep in missing:
            print(f"  ‚Ä¢ {{dep}}")
        
        print("\\n Installation Instructions:")
        print("\\n1. Install PLIP:")
        print("   pip install plip")
        print("\\n2. Install Open Babel:")
        print("   - Ubuntu/Debian: sudo apt-get install openbabel")
        print("   - macOS: brew install open-babel")
        print("   - Windows: Download from http://openbabel.org/wiki/Category:Installation")
        print("   - Or via conda: conda install -c conda-forge openbabel")
        print("\\n" + "="*80)
        sys.exit(1)
    
    print("="*80)
    print("‚úîÔ∏è All dependencies satisfied!\\n")



# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def safe_run(cmd, cwd=None):
    """Run command and handle errors"""
    p = subprocess.run(cmd, cwd=cwd, stdout=subprocess.PIPE, 
                      stderr=subprocess.PIPE, text=True)
    if p.returncode != 0:
        raise Exception(f"Failed: {{' '.join(cmd)}}\\n{{p.stderr}}")
    return p.stdout


def remove_smiles_from_pdb(pdb_path):
    """Remove SMILES strings from PDB file"""
    temp_path = pdb_path + ".tmp"
    lines_removed = 0
    
    try:
        with open(pdb_path, "r") as infile, open(temp_path, "w") as outfile:
            for line in infile:
                stripped = line.strip()
                is_smiles = False
                
                if stripped.startswith("SMILES") or stripped.lower().startswith("smiles:"):
                    is_smiles = True
                elif "[C@@H]" in line or "[C@H]" in line or "[@" in line:
                    is_smiles = True
                elif stripped and not stripped.startswith(("ATOM", "HETATM", "TER", "END", 
                                                          "MODEL", "ENDMDL", "CONECT", 
                                                          "REMARK", "HEADER", "TITLE", "CRYST")):
                    if len(stripped) > 50 and stripped.count("(") + stripped.count("[") > 5 and stripped.count(" ") < 3:
                        is_smiles = True
                
                if is_smiles:
                    lines_removed += 1
                    continue
                    
                outfile.write(line)
        
        shutil.move(temp_path, pdb_path)
        
        if lines_removed > 0:
            print(f"    ‚úì Removed {{lines_removed}} SMILES lines from PDB")
        
        return True
    except Exception as e:
        print(f"    ‚ö† Warning: Could not clean SMILES from PDB: {{e}}")
        if os.path.exists(temp_path):
            os.remove(temp_path)
        return False


def split_pdbqt_models(src, out_dir):
    """Split multi-model PDBQT into individual pose files"""
    poses = []
    model = []
    idx = 0
    with open(src, "r") as f:
        for line in f:
            if line.startswith("MODEL"):
                model = []
            model.append(line)
            if line.startswith("ENDMDL"):
                idx += 1
                out = os.path.join(out_dir, f"pose_{{idx}}.pdbqt")
                with open(out, "w") as o:
                    o.writelines(model)
                poses.append(out)
                model = []
    if model:
        idx += 1
        out = os.path.join(out_dir, f"pose_{{idx}}.pdbqt")
        with open(out, "w") as o:
            o.writelines(model)
        poses.append(out)
    return poses


def merge_pdbqt(rec, lig, outp):
    """Merge receptor and ligand into single PDB file"""
    with open(outp, "w") as out:
        out.write(open(rec).read())
        out.write(open(lig).read())


def parse_plip_xml(xml_path, output_dir):
    """Parse PLIP XML output and create CSV/JSON files"""
    if not os.path.exists(xml_path):
        return False
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()
        all_interactions = []
        interactions_by_type = {{}}
        
        for bindingsite in root.findall('.//bindingsite'):
            interactions_node = bindingsite.find('interactions')
            if not interactions_node:
                continue
            identifiers = bindingsite.find('identifiers')
            site_info = {{
                'ligand_id': identifiers.find('hetid').text if identifiers.find('hetid') is not None else '',
                'chain': identifiers.find('chain').text if identifiers.find('chain') is not None else '',
                'position': identifiers.find('position').text if identifiers.find('position') is not None else ''
            }}
            
            interaction_map = {{
                'hydrophobic_interactions': 'hydrophobic_interaction',
                'hydrogen_bonds': 'hydrogen_bond',
                'water_bridges': 'water_bridge',
                'salt_bridges': 'salt_bridge',
                'pi_stacks': 'pi_stack',
                'pi_cation_interactions': 'pi_cation_interaction',
                'halogen_bonds': 'halogen_bond',
                'metal_complexes': 'metal_complex'
            }}
            
            for plural, singular in interaction_map.items():
                coll = interactions_node.find(plural)
                if not coll:
                    continue
                interaction_type_name = plural.replace('_', ' ').title()
                if interaction_type_name not in interactions_by_type:
                    interactions_by_type[interaction_type_name] = []
                for interaction in coll.findall(singular):
                    data = {{**site_info, 'interaction_type': interaction_type_name}}
                    for child in interaction:
                        if child.text and child.text.strip():
                            data[child.tag] = child.text.strip()
                        elif len(child) > 0:
                            for subchild in child:
                                if subchild.text and subchild.text.strip():
                                    data[f"{{child.tag}}_{{subchild.tag}}"] = subchild.text.strip()
                    all_interactions.append(data)
                    interactions_by_type[interaction_type_name].append(data)
        
        if all_interactions:
            csv_path = os.path.join(output_dir, 'interactions_all.csv')
            preferred_order = [
                'interaction_type', 'dist', 'dist_d-a', 'dist_h-a', 'don_angle', 'donoridx',
                'donortype', 'restype', 'resnr', 'acceptoridx', 'acceptortype',
            ]
            all_keys = sorted(set(k for d in all_interactions for k in d.keys()))
            ordered_keys = [k for k in preferred_order if k in all_keys] + \\
                           [k for k in all_keys if k not in preferred_order]
            keys = ordered_keys

            with open(csv_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=keys)
                writer.writeheader()
                writer.writerows(all_interactions)
            
            for itype, interactions in interactions_by_type.items():
                if not interactions:
                    continue
                filename = f"{itype.replace(' ', '_')}.csv"
                with open(os.path.join(output_dir, filename), 'w', newline='', encoding='utf-8') as f:
                    type_keys = sorted(set(k for d in interactions for k in d.keys()))
                    writer = csv.DictWriter(f, fieldnames=type_keys)
                    writer.writeheader()
                    writer.writerows(interactions)
            
            # Save all interactions to JSON
            with open(os.path.join(output_dir, 'interactions_all.json'), 'w') as f:
                json.dump(all_interactions, f, indent=2)
            
            # Skip specific interaction type JSON files
            skip_json_types = ['Hydrogen Bonds', 'Hydrophobic Interactions', 'Salt Bridges']
            
            for itype, interactions in interactions_by_type.items():
                if not interactions:
                    continue
                
                # Skip the three specific JSON files
                if itype in skip_json_types:
                    continue
                
                filename = f"{{itype.replace(' ', '_')}}.json"
                with open(os.path.join(output_dir, filename), 'w') as f:
                    json.dump(interactions, f, indent=2)
                    
            summary_path = os.path.join(output_dir, 'interaction_summary.csv')
            counts = {{}}
            for i in all_interactions:
                itype = i.get('interaction_type', 'Unknown')
                counts[itype] = counts.get(itype, 0) + 1
            with open(summary_path, 'w', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(['Interaction Type', 'Count'])
                for itype, count in sorted(counts.items()):
                    writer.writerow([itype, count])
            return True
        return False
    except Exception as e:
        print(f"    ‚ö† Error parsing XML: {{e}}")
        return False

def aggregate_plip_results_v2(job_dir):
    """
    Two-phase aggregation:
    Phase 1: Create all_poses_interactions.csv for each combination
    Phase 2: Read from all_poses_interactions.csv to generate summaries
    """
    
    print("="*80)
    print("PHASE 1: Creating all_poses_interactions.csv for each combination")
    print("="*80)
    
    # Find all combination folders
    combo_folders = []
    for root, dirs, files in os.walk(job_dir):
        if "original_files" in dirs and "poses" in dirs:
            combo_folders.append(root)
    
    print(f"Found {{len(combo_folders)}} combination folders\\n")
    
    # Phase 1: Create all_poses_interactions.csv for each combination
    for idx, combo_folder in enumerate(combo_folders, 1):
        combo_name = os.path.basename(combo_folder)
        print(f"[{{idx}}/{{len(combo_folders)}}] Processing: {{combo_name}}")
        
        # Find all pose folders
        pose_folders = []
        for item in os.listdir(combo_folder):
            item_path = os.path.join(combo_folder, item)
            if os.path.isdir(item_path) and item.startswith("pose_"):
                pose_folders.append(item_path)
        
        # Collect all interactions from all poses
        all_plip_interactions = []
        
        for pose_folder in sorted(pose_folders):
            pose_num = int(os.path.basename(pose_folder).replace('pose_', ''))
            interactions_csv = os.path.join(pose_folder, "interactions_all.csv")
            
            if os.path.exists(interactions_csv):
                with open(interactions_csv, 'r', newline='', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        row_with_pose = {{'Pose': pose_num}}
                        row_with_pose.update(row)
                        all_plip_interactions.append(row_with_pose)
        
        # Write all_poses_interactions.csv
        if all_plip_interactions:
            all_poses_csv_path = os.path.join(combo_folder, 'all_poses_interactions.csv')
            with open(all_poses_csv_path, 'w', newline='', encoding='utf-8') as f:
                # Collect ALL unique fieldnames from all interactions
                all_fields = set()
                for interaction in all_plip_interactions:
                    all_fields.update(interaction.keys())
                all_fieldnames = ['Pose'] + sorted([f for f in all_fields if f != 'Pose'])
                writer = csv.DictWriter(f, fieldnames=all_fieldnames)
                writer.writeheader()
                writer.writerows(all_plip_interactions)
            
            # Also create JSON version
            all_poses_json_path = os.path.join(combo_folder, 'all_poses_interactions.json')
            with open(all_poses_json_path, 'w', encoding='utf-8') as f:
                json.dump(all_plip_interactions, f, indent=2)
            
            print(f"  Created all_poses_interactions.csv ({{len(all_plip_interactions)}} interactions)")
        else:
            print(f" ‚ö†Ô∏è  No interactions found")
    
    print("\\n" + "="*80)
    print(" PHASE 2: Generating summary files from all_poses_interactions.csv")
    print("="*80 + "\\n")
    
    # Phase 2: Read from all_poses_interactions.csv and generate summaries
    all_interactions = []
    ligand_data = {{}}
    residue_stats = {{}}
    interaction_type_stats = {{}}
    ligand_residue_matrix = {{}}
    
    print("Reading all_poses_interactions.csv files...")
    
    for combo_folder in combo_folders:
        combo_name = os.path.basename(combo_folder)
        all_poses_csv = os.path.join(combo_folder, 'all_poses_interactions.csv')
        
        if not os.path.exists(all_poses_csv):
            continue
        
        # Extract ligand name from combo name
        ligand_name = combo_name.split('_', 1)[-1] if '_' in combo_name else combo_name
        
        with open(all_poses_csv, 'r', newline='', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            
            for row in reader:
                pose_num = int(row.get('Pose', 1))
                residue = f"{{row.get('restype', '')}}{{row.get('resnr', '')}}"
                interaction_type_raw = row.get('interaction_type', 'Unknown')
                
                # Normalize interaction type
                interaction_type = interaction_type_raw.replace('_', ' ').title()
                
                # Simplified type names
                type_short = 'Other'
                if 'Hydrogen' in interaction_type:
                    type_short = 'H-bond'
                elif 'Salt' in interaction_type:
                    type_short = 'Salt-bridge'
                elif 'Pi Stack' in interaction_type:
                    type_short = 'Pi-stack'
                elif 'Hydrophobic' in interaction_type:
                    type_short = 'Hydrophobic'
                elif 'Pi Cation' in interaction_type:
                    type_short = 'Pi-cation'
                elif 'Halogen' in interaction_type:
                    type_short = 'Halogen'
                elif 'Water' in interaction_type:
                    type_short = 'Water-bridge'
                elif 'Metal' in interaction_type:
                    type_short = 'Metal'
                
                # Store interaction record
                interaction_record = {{
                    'ligand': ligand_name,
                    'pose': pose_num,
                    'residue': residue,
                    'interaction_type': type_short,
                    'distance': row.get('dist', row.get('dist_h-a', row.get('dist_d-a', ''))),
                }}
                all_interactions.append(interaction_record)
                
                # Update ligand_data
                if ligand_name not in ligand_data:
                    ligand_data[ligand_name] = {{}}
                if pose_num not in ligand_data[ligand_name]:
                    ligand_data[ligand_name][pose_num] = []
                ligand_data[ligand_name][pose_num].append(interaction_record)
                
                # Update residue stats
                if residue not in residue_stats:
                    residue_stats[residue] = {{}}
                residue_stats[residue][type_short] = residue_stats[residue].get(type_short, 0) + 1
                
                # Update interaction type stats
                if type_short not in interaction_type_stats:
                    interaction_type_stats[type_short] = {{
                        'total_count': 0,
                        'ligands': set(),
                        'residues': {{}}
                    }}
                interaction_type_stats[type_short]['total_count'] += 1
                interaction_type_stats[type_short]['ligands'].add(ligand_name)
                interaction_type_stats[type_short]['residues'][residue] = \
                    interaction_type_stats[type_short]['residues'].get(residue, 0) + 1
                
                # Update ligand-residue matrix
                if ligand_name not in ligand_residue_matrix:
                    ligand_residue_matrix[ligand_name] = {{}}
                if residue not in ligand_residue_matrix[ligand_name]:
                    ligand_residue_matrix[ligand_name][residue] = {{}}
                ligand_residue_matrix[ligand_name][residue][type_short] = \
                    ligand_residue_matrix[ligand_name][residue].get(type_short, 0) + 1
    
    if not all_interactions:
        print("  ‚ö†Ô∏è  No interaction data found!")
        return
    
    print(f" Collected {{len(all_interactions)}} interactions from {{len(ligand_data)}} ligands\\n")
    
    # Generate all summary files (same as before)
    print(" Generating summary files...")
    
    # FILE 1: residue_summary.csv
    print("  Creating residue_summary.csv...")
    residue_summary = []
    for residue, type_counts in residue_stats.items():
        total_interactions = sum(type_counts.values())
        ligands_with_residue = set()
        for ligand, residues in ligand_residue_matrix.items():
            if residue in residues:
                ligands_with_residue.add(ligand)
        
        ligand_count = len(ligands_with_residue)
        ligand_coverage = (ligand_count / len(ligand_data)) * 100 if ligand_data else 0
        
        residue_summary.append({{
            'Residue': residue,
            'Total_Interactions': total_interactions,
            'Ligand_Count': ligand_count,
            'Ligand_Coverage_%': round(ligand_coverage, 1),
            'H-bond': type_counts.get('H-bond', 0),
            'Salt-bridge': type_counts.get('Salt-bridge', 0),
            'Pi-stack': type_counts.get('Pi-stack', 0),
            'Hydrophobic': type_counts.get('Hydrophobic', 0),
            'Pi-cation': type_counts.get('Pi-cation', 0),
            'Halogen': type_counts.get('Halogen', 0),
            'Water-bridge': type_counts.get('Water-bridge', 0),
            'Metal': type_counts.get('Metal', 0),
        }})
    
    residue_summary.sort(key=lambda x: x['Total_Interactions'], reverse=True)
    
    summary_path = os.path.join(job_dir, 'residue_summary.csv')
    with open(summary_path, 'w', newline='', encoding='utf-8') as f:
        fieldnames = ['Residue', 'Total_Interactions', 'Ligand_Count', 'Ligand_Coverage_%',
                     'H-bond', 'Salt-bridge', 'Pi-stack', 'Hydrophobic', 
                     'Pi-cation', 'Halogen', 'Water-bridge', 'Metal']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(residue_summary)
    print(f"  residue_summary.csv ({{len(residue_summary)}} residues)")
    
    # FILE 2: ligand_interaction_matrix_top100.csv
    print("  Creating ligand_interaction_matrix_top100.csv...")
    all_residues = [r['Residue'] for r in residue_summary[:50]]
    
    ligand_summary = []
    for ligand, residue_data in ligand_residue_matrix.items():
        total_interactions = sum(sum(types.values()) for types in residue_data.values())
        unique_residues = len(residue_data)
        
        row = {{
            'Ligand': ligand,
            'Total_Interactions': total_interactions,
            'Unique_Residues': unique_residues,
        }}
        
        for residue in all_residues:
            if residue in residue_data:
                row[residue] = sum(residue_data[residue].values())
            else:
                row[residue] = 0
        
        ligand_summary.append(row)
    
    ligand_summary.sort(key=lambda x: x['Total_Interactions'], reverse=True)
    top100_ligands = ligand_summary[:100]
    
    top100_path = os.path.join(job_dir, 'ligand_interaction_matrix_top100.csv')
    with open(top100_path, 'w', newline='', encoding='utf-8') as f:
        fieldnames = ['Ligand', 'Total_Interactions', 'Unique_Residues'] + all_residues
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(top100_ligands)
    print(f"  ligand_interaction_matrix_top100.csv ({{len(top100_ligands)}} ligands)")
    
    # FILE 3: ligand_summary_with_top_residues.csv
    print("  Creating ligand_summary_with_top_residues.csv...")
    ligand_summary_detailed = []
    for ligand, residue_data in ligand_residue_matrix.items():
        total_interactions = sum(sum(types.values()) for types in residue_data.values())
        unique_residues = len(residue_data)
        
        h_bond_total = sum(types.get('H-bond', 0) for types in residue_data.values())
        salt_total = sum(types.get('Salt-bridge', 0) for types in residue_data.values())
        pi_stack_total = sum(types.get('Pi-stack', 0) for types in residue_data.values())
        hydrophobic_total = sum(types.get('Hydrophobic', 0) for types in residue_data.values())
        pi_cation_total = sum(types.get('Pi-cation', 0) for types in residue_data.values())
        halogen_total = sum(types.get('Halogen', 0) for types in residue_data.values())
        water_total = sum(types.get('Water-bridge', 0) for types in residue_data.values())
        metal_total = sum(types.get('Metal', 0) for types in residue_data.values())
        
        residue_totals = [(res, sum(types.values())) for res, types in residue_data.items()]
        residue_totals.sort(key=lambda x: x[1], reverse=True)
        
        top_res_1 = residue_totals[0] if len(residue_totals) > 0 else ('', 0)
        top_res_2 = residue_totals[1] if len(residue_totals) > 1 else ('', 0)
        top_res_3 = residue_totals[2] if len(residue_totals) > 2 else ('', 0)
        
        ligand_summary_detailed.append({{
            'Ligand': ligand,
            'Total_Interactions': total_interactions,
            'Unique_Residues': unique_residues,
            'Top_Residue_1': top_res_1[0],
            'Top_Res_1_Count': top_res_1[1],
            'Top_Residue_2': top_res_2[0],
            'Top_Res_2_Count': top_res_2[1],
            'Top_Residue_3': top_res_3[0],
            'Top_Res_3_Count': top_res_3[1],
            'H-bond_Total': h_bond_total,
            'Salt-bridge_Total': salt_total,
            'Pi-stack_Total': pi_stack_total,
            'Hydrophobic_Total': hydrophobic_total,
            'Pi-cation_Total': pi_cation_total,
            'Halogen_Total': halogen_total,
            'Water-bridge_Total': water_total,
            'Metal_Total': metal_total,
        }})
    
    ligand_summary_detailed.sort(key=lambda x: x['Total_Interactions'], reverse=True)
    
    ligand_summary_path = os.path.join(job_dir, 'ligand_summary_with_top_residues.csv')
    with open(ligand_summary_path, 'w', newline='', encoding='utf-8') as f:
        fieldnames = [
            'Ligand', 'Total_Interactions', 'Unique_Residues',
            'Top_Residue_1', 'Top_Res_1_Count',
            'Top_Residue_2', 'Top_Res_2_Count',
            'Top_Residue_3', 'Top_Res_3_Count',
            'H-bond_Total', 'Salt-bridge_Total', 'Pi-stack_Total', 'Hydrophobic_Total',
            'Pi-cation_Total', 'Halogen_Total', 'Water-bridge_Total', 'Metal_Total'
        ]
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(ligand_summary_detailed)
    print(f" ligand_summary_with_top_residues.csv ({{len(ligand_summary_detailed)}} ligands)")
    
    # FILE 4: interaction_type_breakdown.csv
    print("  Creating interaction_type_breakdown.csv...")
    type_breakdown = []
    for itype, stats in interaction_type_stats.items():
        total_count = stats['total_count']
        ligand_count = len(stats['ligands'])
        avg_per_ligand = total_count / ligand_count if ligand_count > 0 else 0
        
        most_common_res = max(stats['residues'].items(), key=lambda x: x[1]) if stats['residues'] else ('N/A', 0)
        
        type_breakdown.append({{
            'Interaction_Type': itype,
            'Total_Count': total_count,
            'Ligand_Count': ligand_count,
            'Avg_Per_Ligand': round(avg_per_ligand, 1),
            'Most_Common_Residue': most_common_res[0],
            'Residue_Count': most_common_res[1]
        }})
    
    type_breakdown.sort(key=lambda x: x['Total_Count'], reverse=True)
    
    breakdown_path = os.path.join(job_dir, 'interaction_type_breakdown.csv')
    with open(breakdown_path, 'w', newline='', encoding='utf-8') as f:
        fieldnames = ['Interaction_Type', 'Total_Count', 'Ligand_Count', 'Avg_Per_Ligand', 
                     'Most_Common_Residue', 'Residue_Count']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(type_breakdown)
    print(f"  interaction_type_breakdown.csv ({{len(type_breakdown)}} types)")
    
    # FILE 5: best_residue_per_ligand.csv
    print("  Creating best_residue_per_ligand.csv...")
    best_residue_data = []
    for ligand, residue_data in ligand_residue_matrix.items():
        best_residue = None
        best_total = 0
        best_type_counts = {{}}
        
        for residue, type_counts in residue_data.items():
            total = sum(type_counts.values())
            if total > best_total:
                best_total = total
                best_residue = residue
                best_type_counts = type_counts
        
        if best_residue:
            best_residue_data.append({{
                'Ligand': ligand,
                'Best_Residue': best_residue,
                'Total_Interactions': best_total,
                'H-bond': best_type_counts.get('H-bond', 0),
                'Salt-bridge': best_type_counts.get('Salt-bridge', 0),
                'Pi-stack': best_type_counts.get('Pi-stack', 0),
                'Hydrophobic': best_type_counts.get('Hydrophobic', 0),
                'Pi-cation': best_type_counts.get('Pi-cation', 0),
                'Halogen': best_type_counts.get('Halogen', 0),
                'Water-bridge': best_type_counts.get('Water-bridge', 0),
                'Metal': best_type_counts.get('Metal', 0),
            }})
    
    best_residue_data.sort(key=lambda x: x['Total_Interactions'], reverse=True)
    
    best_residue_path = os.path.join(job_dir, 'best_residue_per_ligand.csv')
    with open(best_residue_path, 'w', newline='', encoding='utf-8') as f:
        fieldnames = ['Ligand', 'Best_Residue', 'Total_Interactions', 'H-bond', 'Salt-bridge', 
                     'Pi-stack', 'Hydrophobic', 'Pi-cation', 'Halogen', 'Water-bridge', 'Metal']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(best_residue_data)
    print(f"  best_residue_per_ligand.csv ({{len(best_residue_data)}} ligands)")
    
    # FILE 6: Generate poses_summary.csv and poses_interaction_matrix.csv in each combo folder
    print("\\n  Creating per-combination pose files...")
    for combo_folder in combo_folders:
        combo_name = os.path.basename(combo_folder)
        all_poses_csv = os.path.join(combo_folder, 'all_poses_interactions.csv')
        
        if not os.path.exists(all_poses_csv):
            continue
        
        # Extract ligand name
        ligand_name = combo_name.split('_', 1)[-1] if '_' in combo_name else combo_name
        
        if ligand_name not in ligand_data:
            continue
        
        poses_data = ligand_data[ligand_name]
        pose_matrix = []
        pose_summary_data = []
        
        for pose_num in sorted(poses_data.keys()):
            interactions = poses_data[pose_num]
            
            residue_interactions = {{}}
            for interaction in interactions:
                res = interaction['residue']
                itype = interaction['interaction_type']
                
                if res not in residue_interactions:
                    residue_interactions[res] = {{
                        'H-bond': 0, 'Salt-bridge': 0, 'Pi-stack': 0, 'Hydrophobic': 0,
                        'Pi-cation': 0, 'Halogen': 0, 'Water-bridge': 0, 'Metal': 0
                    }}
                
                residue_interactions[res][itype] += 1
            
            pose_total_interactions = sum(sum(types.values()) for types in residue_interactions.values())
            pose_unique_residues = len(residue_interactions)
            
            pose_h_bond = sum(types['H-bond'] for types in residue_interactions.values())
            pose_salt = sum(types['Salt-bridge'] for types in residue_interactions.values())
            pose_pi_stack = sum(types['Pi-stack'] for types in residue_interactions.values())
            pose_hydrophobic = sum(types['Hydrophobic'] for types in residue_interactions.values())
            pose_pi_cation = sum(types['Pi-cation'] for types in residue_interactions.values())
            pose_halogen = sum(types['Halogen'] for types in residue_interactions.values())
            pose_water = sum(types['Water-bridge'] for types in residue_interactions.values())
            pose_metal = sum(types['Metal'] for types in residue_interactions.values())
            
            best_residue = ''
            best_res_count = 0
            best_res_types = {{'H-bond': 0, 'Salt-bridge': 0, 'Pi-stack': 0, 'Hydrophobic': 0,
                             'Pi-cation': 0, 'Halogen': 0, 'Water-bridge': 0, 'Metal': 0}}
            
            if residue_interactions:
                best_residue = max(residue_interactions.items(), key=lambda x: sum(x[1].values()))[0]
                best_res_count = sum(residue_interactions[best_residue].values())
                best_res_types = residue_interactions[best_residue]
            
            pose_summary_data.append({{
                'Pose': pose_num,
                'Total_Interactions': pose_total_interactions,
                'Unique_Residues': pose_unique_residues,
                'Best_Residue': best_residue,
                'Best_Res_Count': best_res_count,
                'Best_Res_H-bond': best_res_types['H-bond'],
                'Best_Res_Salt-bridge': best_res_types['Salt-bridge'],
                'Best_Res_Pi-stack': best_res_types['Pi-stack'],
                'Best_Res_Hydrophobic': best_res_types['Hydrophobic'],
                'Best_Res_Pi-cation': best_res_types['Pi-cation'],
                'Best_Res_Halogen': best_res_types['Halogen'],
                'Best_Res_Water-bridge': best_res_types['Water-bridge'],
                'Best_Res_Metal': best_res_types['Metal'],
                'Total_H-bond': pose_h_bond,
                'Total_Salt-bridge': pose_salt,
                'Total_Pi-stack': pose_pi_stack,
                'Total_Hydrophobic': pose_hydrophobic,
                'Total_Pi-cation': pose_pi_cation,
                'Total_Halogen': pose_halogen,
                'Total_Water-bridge': pose_water,
                'Total_Metal': pose_metal,
            }})
            
            for residue, type_counts in residue_interactions.items():
                total = sum(type_counts.values())
                pose_matrix.append({{
                    'Pose': pose_num,
                    'Residue': residue,
                    'Total_Interactions': total,
                    'H-bond': type_counts['H-bond'],
                    'Salt-bridge': type_counts['Salt-bridge'],
                    'Pi-stack': type_counts['Pi-stack'],
                    'Hydrophobic': type_counts['Hydrophobic'],
                    'Pi-cation': type_counts['Pi-cation'],
                    'Halogen': type_counts['Halogen'],
                    'Water-bridge': type_counts['Water-bridge'],
                    'Metal': type_counts['Metal'],
                }})
        
        # Write poses_summary.csv
        pose_summary_path = os.path.join(combo_folder, 'poses_summary.csv')
        with open(pose_summary_path, 'w', newline='', encoding='utf-8') as f:
            fieldnames = [
                'Pose', 'Total_Interactions', 'Unique_Residues',
                'Best_Residue', 'Best_Res_Count',
                'Best_Res_H-bond', 'Best_Res_Salt-bridge', 'Best_Res_Pi-stack', 'Best_Res_Hydrophobic',
                'Best_Res_Pi-cation', 'Best_Res_Halogen', 'Best_Res_Water-bridge', 'Best_Res_Metal',
                'Total_H-bond', 'Total_Salt-bridge', 'Total_Pi-stack', 'Total_Hydrophobic',
                'Total_Pi-cation', 'Total_Halogen', 'Total_Water-bridge', 'Total_Metal'
            ]
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(pose_summary_data)
        
        # Write poses_interaction_matrix.csv
        pose_csv_path = os.path.join(combo_folder, 'poses_interaction_matrix.csv')
        with open(pose_csv_path, 'w', newline='', encoding='utf-8') as f:
            fieldnames = ['Pose', 'Residue', 'Total_Interactions', 'H-bond', 'Salt-bridge', 
                         'Pi-stack', 'Hydrophobic', 'Pi-cation', 'Halogen', 'Water-bridge', 'Metal']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(pose_matrix)
    
    print(f"  Created pose files in {{len(combo_folders)}} combination folders")
    
    print("\\n" + "="*80)
    print(" AGGREGATION COMPLETE!")
    print("="*80)
    print(f"  Generated 5 global summary files")
    print(f"  Generated pose files in {{len(combo_folders)}} combination folders")
    print(f"  üóÅ Location: {{job_dir}}")
    print("="*80)

# ============================================================================
# MATCHING LOGIC
# ============================================================================

def get_receptor_identifiers(receptor_filename):
    """Extract possible identifiers from receptor name"""
    base = receptor_filename.replace('.pdbqt', '').replace('.pdb', '')
    identifiers = [base]
    
    if '_' in base:
        parts = base.split('_')
        for i in range(len(parts)):
            prefix = '_'.join(parts[:i+1])
            if prefix and prefix not in identifiers:
                identifiers.append(prefix)
        
        if parts[0] not in identifiers:
            identifiers.append(parts[0])
    
    identifiers.sort(key=len, reverse=True)
    return identifiers


def separate_receptors_and_ligands(all_files):
    """
    Intelligently separate files into receptors and ligands based on patterns
    
    Logic:
    - Ligands (docking results) typically have pattern: receptor_compound_score_...
    - Receptors are simpler: protein_name.pdbqt
    - Ligands usually have more underscores and numeric scores
    """
    receptors = []
    ligands = []
    
    print(f"\\n{{'='*80}}")
    print(f" INTELLIGENT FILE SEPARATION")
    print(f"{{'='*80}}")
    
    for file_path in all_files:
        filename = os.path.basename(file_path)
        base_name = filename.replace('.pdbqt', '').replace('.pdb', '')
        
        # Criteria for identifying ligands (docking results):
        # 1. Has negative numbers (scores like -7.36)
        # 2. Has 3+ underscores (receptor_compound_score_...)
        # 3. Has multiple decimal numbers
        
        has_negative_number = '-' in base_name and any(c.isdigit() for c in base_name.split('-')[-1][:3])
        underscore_count = base_name.count('_')
        decimal_count = base_name.count('.')
        
        # Ligand detection logic
        is_ligand = (
            has_negative_number or  # Has docking score
            underscore_count >= 3 or  # Complex naming with multiple parts
            (underscore_count >= 2 and decimal_count >= 2)  # Has coordinates/scores
        )
        
        if is_ligand:
            ligands.append(file_path)
            print(f"   LIGAND: {{filename}}")
        else:
            receptors.append(file_path)
            print(f"   RECEPTOR: {{filename}}")
    
    print(f"{{'='*80}}")
    print(f"  Total Receptors: {{len(receptors)}}")
    print(f"  Total Ligands: {{len(ligands)}}")
    print(f"{{'='*80}}\\n")
    
    return receptors, ligands


def match_receptors_to_ligands(receptor_files, ligand_files):
    """Match receptors to ligands using flexible prefix matching"""
    combinations = []
    skipped_receptors = []
    
    print(f"{{'='*80}}")
    print(f" MATCHING PROCESS STARTED (Flexible Prefix Matching)")
    print(f"{{'='*80}}")
    print(f"  Total receptors to process: {{len(receptor_files)}}")
    print(f"  Total ligands available: {{len(ligand_files)}}")
    print(f"{{'='*80}}\\n")
    
    for idx, receptor_path in enumerate(receptor_files, 1):
        receptor_name = os.path.basename(receptor_path)
        receptor_base = receptor_name.replace('.pdbqt', '').replace('.pdb', '')
        
        print(f"‚îå{{'‚îÄ'*78}}‚îê")
        print(f"‚îÇ[{{idx}}/{{len(receptor_files)}}] Processing Receptor: {{receptor_name:<60}}‚îÇ")
        print(f"‚îî{{'‚îÄ'*78}}‚îò")
        print(f"  Full path: {{receptor_path}}")
        print(f"  Base name: {{receptor_base}}")
        print(f"  Checking {{len(ligand_files)}} ligand(s)...\\n")
        
        matches_found = 0
        
        for ligand_path in ligand_files:
            ligand_name = os.path.basename(ligand_path)
            ligand_base = ligand_name.replace('.pdbqt', '').replace('.pdb', '')
            
            matched = False
            match_type = ""
            
            # METHOD 1: Exact prefix match with underscore
            if ligand_base.startswith(f"{{receptor_base}}_"):
                matched = True
                match_type = f"exact: '{{receptor_base}}_'"
            
            # METHOD 2: Flexible matching - first 8+ characters match
            elif len(receptor_base) >= 8:
                for min_chars in [12, 11, 10, 9, 8]:  # Try longer matches first
                    if len(receptor_base) >= min_chars:
                        prefix = receptor_base[:min_chars]
                        if ligand_base.startswith(prefix):
                            matched = True
                            match_type = f"flexible: first {{min_chars}} chars '{{prefix}}'"
                            break
            
            if matched:
                combinations.append({{
                    "receptor_path": receptor_path,
                    "receptor_name": receptor_name,
                    "ligand_path": ligand_path,
                    "ligand_name": ligand_name,
                    "matched_identifier": receptor_base
                }})
                matches_found += 1
                print(f"     Match #{{matches_found}}: {{ligand_name}}")
                print(f"       Matched via: {{match_type}}")
        
        print(f"\\n  {{'‚îÄ'*76}}")
        if matches_found == 0:
            print(f"  ‚ö†Ô∏è  NO MATCHES FOUND for {{receptor_name}}")
            print(f"  This receptor will be SKIPPED")
            skipped_receptors.append(receptor_name)
        else:
            print(f"   Total matches for {{receptor_name}}: {{matches_found}}")
        print(f"  {{'‚îÄ'*76}}\\n")
    
    print(f"{{'='*80}}")
    print(f" MATCHING PROCESS COMPLETE")
    print(f"{{'='*80}}")
    print(f"  Total combinations created: {{len(combinations)}}")
    print(f"  Receptors successfully matched: {{len(receptor_files) - len(skipped_receptors)}}")
    print(f"  Receptors skipped (no matches): {{len(skipped_receptors)}}")
    print(f"{{'='*80}}\\n")
    
    if skipped_receptors:
        print(f"‚ö†Ô∏è  The following receptor(s) had NO matching ligands:")
        for i, rec in enumerate(skipped_receptors, 1):
            print(f"    {{i}}. {{rec}}")
        print()
    
    return combinations


def get_all_pdbqt_files(path):
    """Get all .pdb and .pdbqt files from path recursively (file or directory)"""
    # Handle single file
    if os.path.isfile(path):
        if path.endswith(('.pdb', '.pdbqt')):
            print(f"   Single file detected: {{os.path.basename(path)}}")
            return [path]
        else:
            print(f"  ‚ö†Ô∏è  WARNING: {{path}} is not a PDB/PDBQT file")
            return []
    
    # Handle directory - RECURSIVE SEARCH
    if os.path.isdir(path):
        files = []
        try:
            print(f"  üóÅ Searching recursively in: {{path}}")
            for root, dirs, filenames in os.walk(path):
                for fname in filenames:
                    if fname.endswith(('.pdb', '.pdbqt')):
                        full_path = os.path.join(root, fname)
                        files.append(full_path)
                        # Show subdirectory being searched
                        rel_path = os.path.relpath(root, path)
                        if rel_path != '.':
                            print(f"     Found in: {{rel_path}}/{{fname}}")
            
            if files:
                print(f"  Total found: {{len(files)}} PDB/PDBQT file(s) across all subdirectories")
            else:
                print(f"  ‚ö†Ô∏è  No PDB/PDBQT files found in {{path}} or its subdirectories")
            return sorted(files)
        except Exception as e:
            print(f"  ‚úñÔ∏è ERROR reading directory {{path}}: {{e}}")
            return []
    
    # Path doesn't exist - try parent directory
    print(f"  ‚ö†Ô∏è  WARNING: Path does not exist: {{path}}")
    parent = os.path.dirname(path)
    
    if parent and os.path.isdir(parent):
        print(f"   Searching recursively in parent directory: {{parent}}")
        files = []
        for root, dirs, filenames in os.walk(parent):
            for fname in filenames:
                if fname.endswith(('.pdb', '.pdbqt')):
                    full_path = os.path.join(root, fname)
                    files.append(full_path)
        if files:
            print(f"   Found {{len(files)}} PDB/PDBQT file(s) in parent directory tree!")
            return sorted(files)
        else:
            print(f"  ‚úñÔ∏è No PDB/PDBQT files found in parent directory tree")
    
    print(f"  ‚úñÔ∏è ERROR: Could not find any valid files")
    return []

# ============================================================================
# SINGLE COMBINATION PROCESSING
# ============================================================================

def execute_single_combination(combo, max_poses):
    """
    Execute PLIP analysis for a single receptor-ligand combination.

    max_poses semantics:
      - 0 or None ‚Üí analyze ALL poses
      - N > 0     ‚Üí analyze first N poses
    """
    combo_output_dir = combo["output_dir"]

    # Safety: normalize max_poses
    if max_poses is not None and max_poses < 0:
        raise ValueError("max_poses must be >= 0 (0 means all poses)")

    # Copy original files
    original_files_dir = os.path.join(combo_output_dir, "original_files")
    os.makedirs(original_files_dir, exist_ok=True)

    receptor_copy = os.path.join(original_files_dir, combo["receptor_name"])
    ligand_copy = os.path.join(original_files_dir, combo["ligand_name"])

    shutil.copy2(combo["receptor_path"], receptor_copy)
    shutil.copy2(combo["ligand_path"], ligand_copy)

    # Split ligand file into individual poses
    poses_root = os.path.join(combo_output_dir, "poses")
    os.makedirs(poses_root, exist_ok=True)
    all_pose_files = split_pdbqt_models(combo["ligand_path"], poses_root)

    total_poses = len(all_pose_files)

    # --------------------------------
    # Pose selection logic
    # --------------------------------
    if max_poses in (None, 0):
        selected_poses = all_pose_files
        print(f"    Total poses in ligand: {{total_poses}}")
        print(f"    Analyzing ALL poses ({{total_poses}})")
    else:
        selected_poses = all_pose_files[:max_poses]
        print(f"    Total poses in ligand: {{total_poses}}")
        print(f"    Analyzing first {{len(selected_poses)}} pose(s)")

        # Remove unused pose files to save disk space
        for pose_file in all_pose_files[max_poses:]:
            try:
                os.remove(pose_file)
            except Exception:
                pass

    # --------------------------------
    # Process selected poses
    # --------------------------------
    pose_results = []
    for i, pose_file in enumerate(selected_poses, start=1):
        print(f"\n    Processing pose {{i}}/{{len(selected_poses)}}...")
        pose_dir = os.path.join(combo_output_dir, f"pose_{{i}}")
        os.makedirs(pose_dir, exist_ok=True)

        merge_path = os.path.join(pose_dir, f"merge_{{i}}.pdbqt")
        complex_path = os.path.join(pose_dir, "complex.pdb")

        # Merge receptor and ligand
        merge_pdbqt(combo["receptor_path"], pose_file, merge_path)

        # Convert to PDB format
        safe_run(["obabel", merge_path, "-O", complex_path], cwd=pose_dir)

        # Remove SMILES from PDB
        print("      Cleaning SMILES from complex.pdb...")
        remove_smiles_from_pdb(complex_path)

        # Run PLIP
        plip_cmd = [
            "plip", "-f", complex_path, "-x", "-t", "-y",
            "--nohydro", "--nofixfile", "--nofix"
        ]

        try:
            safe_run(plip_cmd, cwd=pose_dir)
            xml_path = os.path.join(pose_dir, "report.xml")
            if os.path.exists(xml_path):
                parse_plip_xml(xml_path, pose_dir)
                print(f"      ‚úì Pose {{i}} completed")
            else:
                print(f"      ‚ö† Pose {{i}}: No XML output")
        except Exception as e:
            print(f"      ‚úó Pose {{i}} failed: {{e}}")

        files = os.listdir(pose_dir)
        pose_results.append({{
            "pose": i,
            "folder": pose_dir,
            "csv_files": [f for f in files if f.endswith(".csv")],
            "json_files": [f for f in files if f.endswith(".json")],
            "png_files": [f for f in files if f.endswith(".png")],
            "xml_files": [f for f in files if f.endswith(".xml")],
            "txt_files": [f for f in files if f.endswith(".txt")],
            "pse_files": [f for f in files if f.endswith(".pse")],
            "pml_files": [f for f in files if f.endswith(".pml")],
            "pdb_files": [f for f in files if f.endswith(".pdb")],
            "pdbqt_files": [f for f in files if f.endswith(".pdbqt")],
        }})

    return {{
        "combo_name": combo["combo_name"],
        "receptor": combo["receptor_name"],
        "ligand": combo["ligand_name"],
        "output_dir": combo_output_dir,
        "total_poses_in_file": total_poses,
        "poses_analyzed": len(selected_poses),
        "poses": pose_results,
    }}



# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    print("\\n" + "="*80)
    print(" PLIP ANALYSIS - STANDALONE SCRIPT")
    print("="*80)
    print("GENERATED BY: COMBI VS  FRAMEWORK 3.0")
    print("="*80)
    
    # Check dependencies first
    check_dependencies()
    
    # Validate paths
    print("\\n" + "="*80)
    print(" VALIDATING PATHS")
    print("="*80)
    
    if not os.path.exists(RECEPTOR_PATH):
        print(f"‚úñÔ∏è Receptor path does not exist: {{RECEPTOR_PATH}}")
        sys.exit(1)
    print(f"  ‚úì Receptor: {{RECEPTOR_PATH}}")
    
    if not os.path.exists(LIGAND_PATH):
        print(f"‚úñÔ∏è Ligand path does not exist: {{LIGAND_PATH}}")
        sys.exit(1)
    print(f"  ‚úì Ligand: {{LIGAND_PATH}}")
    
    # Create output folder
    os.makedirs(OUTPUT_FOLDER, exist_ok=True)
    print(f"  ‚úì Output: {{OUTPUT_FOLDER}}")
    print("="*80)
    
    # Display configuration
    print("\\n" + "="*80)
    print("  CONFIGURATION")
    print("="*80)
    print(f"  Max poses per combination: {{MAX_POSES}}")
    print(f"  Parallel workers: {{MAX_WORKERS}}")
    print(f"  Remove waters: {{REMOVE_WATERS}}")
    print(f"  Remove ions: {{REMOVE_IONS}}")
    print(f"  Add hydrogens: {{ADD_HYDROGENS}}")
    print(f"  Keep heteroatoms: {{KEEP_HETERO}}")
    print("="*80)
    
    # Get receptor and ligand files
    # Get all PDB/PDBQT files recursively
    print("\\n" + "="*80)
    print(" SCANNING FILES RECURSIVELY")
    print("="*80)
    
    # Combine both paths for searching
    search_paths = set()
    if os.path.exists(RECEPTOR_PATH):
        search_paths.add(os.path.abspath(RECEPTOR_PATH))
    if os.path.exists(LIGAND_PATH):
        search_paths.add(os.path.abspath(LIGAND_PATH))
    
    all_files = []
    for path in search_paths:
        files = get_all_pdbqt_files(path)
        all_files.extend(files)
    
    # Remove duplicates while preserving order
    seen = set()
    unique_files = []
    for f in all_files:
        if f not in seen:
            seen.add(f)
            unique_files.append(f)
    all_files = unique_files
    
    if not all_files:
        print("‚úñÔ∏è No PDB/PDBQT files found")
        sys.exit(1)
    
    # Intelligently separate receptors from ligands
    receptor_files, ligand_files = separate_receptors_and_ligands(all_files)
    
    print(f"  Receptors found: {{len(receptor_files)}}")
    for r in receptor_files:
        print(f"    - {{os.path.basename(r)}}")
    
    print(f"  Ligands found: {{len(ligand_files)}}")
    for l in ligand_files:
        print(f"    - {{os.path.basename(l)}}")
    
    print("="*80)
    
    if not receptor_files:
        print("‚úñÔ∏è No receptor files found")
        sys.exit(1)
    if not ligand_files:
        print("‚úñÔ∏è No ligand files found")
        sys.exit(1)
    
    # Match receptors with ligands
    matched_combinations = match_receptors_to_ligands(receptor_files, ligand_files)
    
    if not matched_combinations:
        print("‚úñÔ∏è No matching receptor-ligand pairs found!")
        print("   Ensure ligands are named like: {{receptor_prefix}}_{{compound}}_{{score}}.pdbqt")
        sys.exit(1)
    
    # Create job directory
    job_id = os.urandom(4).hex()
    job_dir = os.path.join(OUTPUT_FOLDER, f"plip_job_{{job_id}}")
    os.makedirs(job_dir, exist_ok=True)
    
    print(f"\\n Job directory created: {{job_dir}}")
    
    # Save matching summary
    summary_path = os.path.join(job_dir, "combinations_summary.json")
    with open(summary_path, 'w') as f:
        json.dump({{
            "total_combinations": len(matched_combinations),
            "combinations": [
                {{
                    "receptor": c["receptor_name"],
                    "ligand": c["ligand_name"],
                    "matched_identifier": c["matched_identifier"]
                }}
                for c in matched_combinations
            ]
        }}, f, indent=2)
    print(f"‚úì Saved combinations summary: {{summary_path}}\\n")
    
    # Prepare output directories for each combination
    for combo in matched_combinations:
        combo_name = f"{{combo['receptor_name'].replace('.pdbqt', '').replace('.pdb', '')}}_{{combo['ligand_name'].replace('.pdbqt', '').replace('.pdb', '')}}"
        combo_dir = os.path.join(job_dir, combo_name)
        os.makedirs(combo_dir, exist_ok=True)
        combo["output_dir"] = combo_dir
        combo["combo_name"] = combo_name
    
    # Execute combinations in parallel
    print(f" Processing {{len(matched_combinations)}} combination(s) in parallel...")
    print(f"   Using {{MAX_WORKERS}} worker(s)\\n")
    
    start_time = time.time()
    results = []
    failed = []
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_combo = {{
            executor.submit(execute_single_combination, combo, MAX_POSES): combo
            for combo in matched_combinations
        }}
        
        for i, future in enumerate(as_completed(future_to_combo), 1):
            combo = future_to_combo[future]
            try:
                result = future.result()
                results.append(result)
                print(f"[{{i}}/{{len(matched_combinations)}}] ‚úî {{combo['combo_name']}}")
            except Exception as e:
                error_info = {{
                    "combo_name": combo['combo_name'],
                    "receptor": combo['receptor_name'],
                    "ligand": combo['ligand_name'],
                    "error": str(e)
                }}
                failed.append(error_info)
                print(f"[{{i}}/{{len(matched_combinations)}}] √ó {{combo['combo_name']}} - {{e}}")
    
    elapsed_time = time.time() - start_time

    print("\\n Aggregating PLIP results across dataset...")
    
    aggregate_plip_results_v2(job_dir)
    # Final summary
    print(f"\\n{{'='*80}}")
    print(f"JOB COMPLETE!")
    print(f"{{'='*80}}")
    print(f"  Job ID: {{job_id}}")
    print(f"  Job Directory: {{job_dir}}")
    print(f"  Total combinations: {{len(matched_combinations)}}")
    print(f"  ‚úì Successful: {{len(results)}}")
    print(f"  √ó Failed: {{len(failed)}}")
    print(f"  ‚è±  Time: {{elapsed_time:.2f}}s")
    print(f"{'='*80}")
    
    if failed:
        print(f"\\n‚ö†Ô∏è  Failed combinations:")
        for f in failed:
            print(f"  - {{f['combo_name']}}: {{f['error']}}")
        print()
    
    # Save final results
    final_results_path = os.path.join(job_dir, "final_results.json")
    final_results = {{
        "job_id": job_id,
        "job_directory": job_dir,
        "start_time": start_time,
        "elapsed_time": elapsed_time,
        "total_combinations": len(matched_combinations),
        "successful": len(results),
        "failed": len(failed),
        "configuration": {{
            "receptor_path": RECEPTOR_PATH,
            "ligand_path": LIGAND_PATH,
            "output_folder": OUTPUT_FOLDER,
            "max_poses": MAX_POSES,
            "max_workers": MAX_WORKERS,
            "remove_waters": REMOVE_WATERS,
            "remove_ions": REMOVE_IONS,
            "add_hydrogens": ADD_HYDROGENS,
            "keep_hetero": KEEP_HETERO
        }},
        "results": results,
        "failed_combinations": failed
    }}
    
    with open(final_results_path, 'w') as f:
        json.dump(final_results, f, indent=2)
    
    print(f"\\n Results saved to: {{final_results_path}}")
    
    # Print summary of output files
    print(f"\\n Output Summary:")
    total_csv = sum(len(pose.get('csv_files', [])) for r in results for pose in r.get('poses', []))
    total_json = sum(len(pose.get('json_files', [])) for r in results for pose in r.get('poses', []))
    total_png = sum(len(pose.get('png_files', [])) for r in results for pose in r.get('poses', []))
    
    print(f"  CSV files: {{total_csv}}")
    print(f"  JSON files: {{total_json}}")
    print(f"  PNG files: {{total_png}}")
    
    print(f"\\n ‚úî Analysis complete! Check the job directory for all results.")
    print(f"   {{job_dir}}\\n")
    print("GENERATED BY: COMBI VS  FRAMEWORK 3.0")

if __name__ == "__main__":
    main()
'''
    
    return script
